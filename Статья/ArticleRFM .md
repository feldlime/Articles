# Не пришей кобыле хвост или как мы прикрутили ML к RFM

## Предыстория

Мы современная высокотехнологичная компания, делаем такую прикольную, никому не понятную штуку -- [Customer Data Platform](https://en.wikipedia.org/wiki/Customer_data_platform) (русского перевода не нашел, но можно сказать, что это сродни "Платформе автоматизации маркетинга").

У нас 2-3 сотни клиентов и есть практически все для удовлетворения их самых неожиданных потребностей: email-mailings, web-push, рекомендации, куча супер-полезных отчетов и многое другое. Но минуточку! А как же [Machine Learning](http://www.machinelearning.ru/wiki/index.php?title=Заглавная_страница)? Он уже проник всюду, а у нас его все еще нет. Нужно срочно исправлять ситуацию и найти несколько подходящих задачек.

Как вы уже догадались, одной из таких задачек стал RFM анализ.

## Пара слов про RFM

Если коротко, то дело было примерно так -- собрались несколько бородатых маркетологов со стажем и обсуждают проблему:
>-- Вы слышали, что индивидуальный подход к клиентам существенно повышает прибыль и снижает отток?

>-- Ага, только если я 500k своих клиентов буду индивидуально обрабатывать, загнусь раньше, чем замечу эффект!

>-- Так ты их на группы раздели, чтобы в каждой похожие чуваки были, тогда только несколько групп будешь индивидуально обрабатывать.

>-- Точно, супер! Только вот... у меня про каждого столько инфы, мне их как лучше группировать, по имени или по фамилии?

>-- Да ты их по RFM группируй, результат что надо будет, точно тебе говорю!


**[RFM](https://en.wikipedia.org/wiki/RFM_(customer_value)** это:

+ *R* (Recency) - насколько давно клиент сделал свой последний заказ;
+ *F* (Frequency) - сколько заказов всего сделал клиент (не думайте про перевод, тут речь не про интервалы);
+ *M* (Monetary) - сколько денег клиент потратил.

Есть еще расширения всякие, но нам пока и этого 3D пространства хватит.

Сейчас многие используют RFM-анализ, о том как они его делают можно почитать [тут](https://habr.com/company/unisender/blog/131225/), ([тут](https://www.putler.com/rfm-analysis/) или [тут](https://www.betaout.com/learn/use-rfm-analysis-segmentation-boost-business-3x/)) -- неплохая визуализация, не могу понять только как она вышла одинаковой у разных компаний... или [вот тут](https://lpgenerator.ru/blog/2016/08/04/kak-uvelichit-prodazhi-segmentaciya-klientov-i-rfm-analiz/). 



Подход у всех схожий.

Сначала клиентов делят на несколько (обычно, три, четыре или пять) групп по каждому признаку, согласно распределению значений давности, частоты и величины покупок.
Четыре группы с тремя переменными образуют 4x4x4 = 64! клиентских сегмента. А если на 5 групп? Посчитали, сколько сегментов? 125!!! 27 или 8, конечно, проще, но про качество можно забыть.

Ну ладно, делить так делить! Стоп. А как делить? Основная сложность -- определить границы групп. В способе их определения как раз и состоит основное отличие между существующими подходами. Что предлагают:  

1.	Разделение на равные по диапазону значений части. Например, если наименьшее количество покупок среди всех покупателей – 1 (одна), наибольшее – 90, а деление происходит на 3 части, то в первой группе окажутся люди, имеющие от 1 до 30 покупок, во второй – от 31 до 60, а в третьей – от 61 до 90.
  + Легко автоматизировать;
  + Более или менее адекватно выявляются VIPы;
  + Все остальное плохо, т.к. распределение, как правило, неравномерно: в нашем примере в первой группе может оказаться 99% клиентов, во второй – 0,99%, а в третьей – 0,01%, поэтому бОльшая часть клиентов оказывается неразделенной.
2.	Разделение на равные по количеству объектов группы – квантили (квартили / квинтили). Применяется чаще предыдущего. 
  + Также легко автоматизировать;
  + Группы более «разумные»; 
  + Однако недостатков по-прежнему больше, чем достоинств – как правило, на самом деле «лучших» клиентов (например, покупающих чаще всего) значительно (на несколько порядков) меньше чем худших, но здесь они не выделяются правильно в отдельную группу.  
3.	Ручной подход. Специалист с аналитическими навыками изучает базу данных и подбирает правильное разделение. 
  + Неплохие результаты;
  + Огромные временнЫе затраты и необходимость наличия специалиста.

Как вы понимаете, мы плохого не делаем, поэтому первые 2 варианта отметаем сразу, а последний реализовать нереально -- многовато аналитиков потребуется (у нас ведь не один проект).

Когда сегменты выделены, нужно придумать, что с ними делать -- какие маркетинговые кампании запустить (ну не для красоты же мы их выделяли). Кстати, вы можете придумать 125 кампаний для сегментов? Нет? А 64, хотя бы? Тоже нет? Ну ничего, никто не может.
Обычно выделяются наиболее важные сегменты с точки зрения специалистов-маркетологов (5-15 штук) – как правило, это крайние сегменты. И для них уже предлагаются кампании по увеличению их маркетинговой ценности.

**Что нам во всем этом процессе не нравится:**

1. Очевидно, основное -- процесс разделения на сегменты. Тут явно требуется что-нибудь новенькое.
2. Количество сегментов. Не очень здорово, что большинство клиентов остаются без внимания.
3. Маркетолог должен вручную подбирать кампании. Зачем напрягать человека, пускай бы лучше машинка это делала...

Ну, а что нам не нравится, мы будем исправлять. По крайней мере, отчасти.



## Новый подход к RFM анализу

В общем, сами собой напросились под эту задачу методы кластеризации -- это методы машинного обучения, которые относятся к классу "обучение без учителя". Без учителя потому, что имея под рукой много данных, с ними нужно что-то сделать, а как это делать никто не объяснил.


**P.S.**
Вот я сейчас рассказываю и вроде как намекаю, что никто до нас этого не делал, ну или, по крайней мере, не говорил об этом. На самом деле это не так -- удалось откопать одну [статью](http://www.kimberlycoffey.com/blog/2016/8/k-means-clustering-for-customer-segmentation), в которой автор проводит научное иследование на эту тему. Но, как мы поняли на собственном опыте, от науки до бизнеса совсем не один шаг.

В общем, была у нас идея и мы наивно думали, что идея -- это полдела. Как же мы ошибались.
Но обо всем по порядку.

### О данных
Опущу историю о том как достать данные, будем считать,что они уже есть.
То есть для каждого клиента *магазина* имеются следующие данные:

* R - количество дней (дробное), прошедшее с момента последней успешной (не отмененной и т.п.) покупки;
* F - количество совершенных покупок (заказов) - точнее, дней с заказами;
* M - общая потраченная сумма.

На самом деле такие данные есть не для всех клиентов, а только для тех, кто что-то купил (ну, а иначе, для тех, кто ничего не купил, какое значение будет у R?).
А как быть с клиентами без покупок? Можно, конечно, выделить их в отдельный сегмент. Но, как выяснилось, у разных магазинов их количество может достигать 20, 50, 80, 95 или даже 100 процентов от общего числа (хотя, конечно, последним наш RFM вряд ли поможет). Так что не помешает их тоже разделить -- по единственному признаку, которым мы можем их описать -- времени жизни (нет, я не про возраст, я про время нахождения в БД).


### Задача

Нужно сделать штуку, которая сможет самостоятельно выделять RFM сегменты в основных данных и D (от Duration) сегменты в данных о клиентах без покупок (D-данных).

Требования такие:

* Сегментов должно быть не мало и не много, в пределах 3-15 шт. в основной части и 1-5 шт. в D-части (ну чисто из соображений здравого смысла и удобства);
* Сегменты должны быть прямоугольными, т.е. их границы д.б. параллельны осям координат. Или состоять из нескольких таких прямоугольных блоков (нескольких, не значит 100500+). Во-первых, это требование системы -- иначе их невозможно будет выделить. Во-вторых так понятнее для человека.


## Поехали

### Подготовка

Ясное дело, прежде всего данные нужно очистить (ну, типа все величины неотрицательные). Об этом не буду подробно, скажу лишь, что обычно все было хорошо и лишь изредка на данном этапе отбрасывались 2-3 экземпляра.

Обычно данные из базы примерно такие (здесь и далее использую 2D графику вместо 3D для понятности, а также не указываю размерности, т.к. особого значения они не имеют): ==картинка==
А хотелось бы такие: ==картинка==
Иначе вон тот чувак, который купил на 8млн станет отдельным кластером...

Нужно удалять выбросы. Проще всего просто обрезать концы (ну, по 2 перцентиля сверху, скажем).

Можно так и оставить, но мы предпочли применить умный метод (в качестве него выбрали [Local Outlier Factor](http://scikit-learn.org/stable/auto_examples/neighbors/plot_lof.html#sphx-glr-auto-examples-neighbors-plot-lof-py)). Этот нехитрый алгоритм сравнивает плотность каждого объекта с плотностям его соседей (а плотность объекта обратно пропорциональна расстоянию до ближайших соседей). Те, у кого это отличие наиболее значительно, считаются аномалиями. Только надо не забыть нормировать как-нибудь данные, а то он вас не поймет (ну, скажем, по M разница между соседями 1000, а по F -- 1, нехорошо).

Обычно он помогает и оставляет после себя чистые данные. Но не всегда. На D-части от него вообще толку немного, а зависнуть может, так что там без него -- просто обрезаем правый конец немного.

Когда все это сделали, проверяем, что данные годятся для сегментации. Проверочка субъективная вышла:
* Число клиентов с покупками -- не меньше 25000. Понятно, что ограничивать надо, иначе ерунда может получиться. 
* Число заказов -- не меньше 50000. Вот тут, пожалуй, стоит смягчить.
* Последняя покупка -- не больше 1,5 дней назад (ну не будем же мы дохлые проекты сегментировать -- только электроэнергию понапрасну тратить).

### Кластеризация

Наконец, дошли до самого интересного.

Имеем хорошие данные, собираемся их кластеризовывать.
Кластеризация -- это поиск структуры в данных.

Тут раздолье -- алгоритмов целая куча. Вот [здесь](http://scikit-learn.org/stable/modules/clustering.html) уже 9 штук, и это далеко не все! Но я решил ограничиться ими, как самыми популярными.

Что ж, сравним.

Понятно, что когда кластеры -- это отдельные шарики, разделить их труда не составляет.
А как насчет чего поинтереснее?
Казалось бы, те, кто справляется с задачей разделения колечек (верхняя линия на картинке), самые лучшие. Но беда в том, что у нас не колечки, у нас месиво какое-то (см. выше). Поиск структуры, говорите...

В общем, мы честно перепробовали все эти алгоритмы. Те из них, которые количество кластеров подбирают сами дают либо порядка 1000 штук, либо вообще 1 или 2. Ну да, это вам не колечки разделять. Про время работы и требования к памяти некоторых я вообще молчу. А про количество [гиперпараметров](https://www.quora.com/What-are-hyperparameters-in-machine-learning), которые непонятно как настраивать, тем более.

Возьмем алгоритмы попроще -- те, что спрашивают у нас, сколько кластеров найти. Проблемы с гиперпараметрами и ресурсами  у многих остаются. (Я на полном серьезе: 50k сэмплов -- это максимум, на что хватает моих 16Гб памяти! При этом время кластеризации ~5-10 минут. Скажете, сервер мощнее? Да, только 50k -- это игрушечный пример, что делать, когда будет хотя бы 500k?)
Но вот беда: многие из них плотностные, т.е. ищут кластеры как плотные сгустки объектов. Что же в этом плохого, спросите вы? По большому счету ничего, они как раз для этого создавались, и это правильно. Только вот, если помните, нам нужны будут прямоугольные сегменты, а плотные сгустки могут иметь форму весьма странную, и я ума не приложу, как распихивать эти загогулины по прямоугольнкам. 
==картинка==

В конце концов после недели бесплодных экспериментов мы пришли к выводу, что не найдем ничего лучше, чем k-means. Да, тот самый простой и банальный k-means, который изучают на вводных курсах по ML. Его прелесть в том, что он не ищет загогулин, а просто группирует точки вокруг центров. И это работает.


Писать свой k-means мы не стали, а использовали версию [scikit-learn](http://scikit-learn.org/stable/). А еще там есть модификация MiniBatchKmeans. Эта штука очень похожа на [SGD (Stochastic Gradient Descent)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Авторы в своем [эксперименте](http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py) показывают, что качество почти не падает, зато время сокращается очень существенно (этого нет в эксперименте, но это так).

В принципе, получаемые результаты были вполне разумными. Но была проблема: k-means -- вещь очень неустойчивая (а MiniBatchKMeans и подавно). Неустойчивость заключается в том, что при повторных запусках результаты могут быть разными, т.к. используется элемент случайности. Можно, конечно, зафиксировать генератор случайных чисел, но основная проблема не в этом. Проблема в том, что если завтра хотя бы один из клиентов совершит хотя бы одну новую покупку, то расположение кластеров может существенно измениться.

Решение, которое приходит в голову и которое предлагает интернет, -- запускать кластеризацию повторно. Самый надежный вариант: запускать столько раз, сколько объектов, по очереди исключая 1 объект из выборки. Но когда объектов пара сотен тысяч, процесс может слегка затянуться. Можно исключать не по одному объекту, а какую-то часть, скажем, 1%.
Мы почти так и сделали -- только ничего не исключали, а брали разные random state и запускали 100 раз. MiniBatchKmeans работает настолько быстро, что даже на самой объемной выборке -- 2 млн объектов -- эти 100 запусков отрабатывают за несколько минут.

После того, как получены 100 вариантов расположения центров кластеров, они все высыпаются в пространство (например, если делим на 12 кластеров, то это 1200 точек). Обычно они образуют неплохо различимые сгустки, а потому их можно смело кластеризовать тем же k-means. Когда получены кластеры, берем медиану в каждом из них и считаем ее реальным центром кластера. После чего просто определяем, какая точка к какому центру ближе. Весь процесс для отмасштабированных данных представлен ниже. ==картинка==

Еще один нерешенный вопрос -- как определить количество кластеров. Есть много [метрик](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation), которые показывают качество разбиения. Часть из них используют знания об истинном разбиении -- это не для реальных задач. Остальные готовы работать и без этого.
Но из-за структуры данных и ограничения на число кластеров эти метрики (например, популярный [Silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)) выдавали результаты не вполне адекватные. 

Поэтому выбор снова пал на самое простое, что можно придумать - так называемый метод "колена". Сначала считается сумма квадратов расстояний от точек до центроидов кластеров, к которым они относятся (формулы приводить не буду). Понятно, что чем она меньше, тем лучше. Но вот только минимум достигается, когда каждая точка являет собой отдельный кластер. Поэтому, обычно, выбирают такое число кластеров, где на графике наблюдается наибольший излом - "колено". В этой точке происходит значительное улучшение качества, значит, вероятно, это оптимальное число кластеров.

На самом деле, в данной задаче в этом методе нет особой необходимости, а потому мы его иногда выключаем и делим просто на 12 кластеров. Практика показывает, что результаты, обычно, неплохие. А количество кластеров, кстати, может уменьшиться позже. Об этом в следующей части. 

   
### Прямоуголизация

Все границы кластеров -- прямые линии, но их наклон может быть произвольным. Выше говорилось, что кластеры должны быть прямоугольными (иметь форму параллелепипедов, грани которых параллельны координатным плоскостям).

То есть нужно вписать кривые сегменты с предыдущей картинки в прямоугольники. 

Чтобы выполнить эту задачу мы использовали [Дерево Решений](https://ru.wikipedia.org/wiki/Дерево_решений) (Decision Tree). Этот достаточно простой метод разбивает все пространство на области с помощью условий по каждой оси (например, x > 1 and y < 10 -- область 1 и т.д.). Границы у получающихся областей как раз такие, как нам нужно. Совокупность всех условий образует бинарное дерево, в котором двигаясь от корня к листьям и проверяя выполнение условий можно прийти в любую нужную область.

Алгоритмы, реализующие решающее дерево -- это алгоритмы машинного обучения, они принимают на вход множество точек и метки, которые определяют, к какому классу каждая точка относится. Затем пространство разделяется так, чтобы как можно больше точек с одинаковыми метками оказались в одной области. У нас в качестве меток выступали метки кластеров, полученные на предыдущем этапе.
 
Можно ли сделать так, чтобы все точки оказались в нужных областях? Разумеется, да. Но выглядеть это будет примерно вот так: ==картинка==

Поэтому нужно ограничивать такие параметры как глубина дерева и число листьев. А то, насколько хорошо их ограничили, проверяется на кросс-валидации -- это процесс, когда дереву для обучения дается только часть объектов, скажем, 2/3, а на остальных проверяется качество. Если дерево будет слишком точно отделять объекты при обучении, то при тестировании новые объекты будут часто попадать в чужие области и качество будет низким. Поэтому качество обычно оказывается высоким при небольших глубине и числе листьев, что нам вполне подходит -- чем меньше областей, тем проще их понять. В качестве метрики качества используем [F-score](https://en.wikipedia.org/wiki/F1_score).

Как происходит чудесное преобразование и какое при этом получается дерево:==картинка==

Как говорилось выше, количество кластеров может уменьшиться: поскольку мы ограничиваем дерево, оно выбирает только наиболее важные области, а если в каком-то кластеров было слишком мало объектов, он просто перестает существовать.

### Постобработка

На этом, в общем-то, все. У нас есть список сегментов с их границами. Среди них есть сегменты людей с покупками и без. Еще есть сегменты с выбросами, но для простоты мы их пока распихиваем по основным сегментам.

Готовые сегменты можно можно просто выдать списочком, чтобы кто-нибудь смотрел и ~~радовался~~ ~~парился~~ придумывал крутые маркетинговые кампании для них. Собственно, это и сделали, добавив краткую статистику типа размера сегмента и среднего чека в нем.

На самом деле, сделали еще супер-крутую штуку -- описание сегментов. Специальная функция смотрит умными глазами на сегменты, берет в руки словарик на 10 листов и описывает каждый сегмент на живом русском языке, чтобы люди не испытывали тоску, глядя на бездушные цифры. Штука получилась настолько крутая, что мы никому ее пока не показываем. Само собой, рассказывать как она устроена, я тоже не буду.

### Тестирование

Проверить, хорошо ли разделены клиенты, задача не из легких.

Самая лучшая метрика для нас -- маркетинговый эффект. Чтобы его измерить, нужно проести А/Б тестирование. Например, так: разделить всех клиентов пополам, одной половине продолжать рассылать одинаковые письма, а другой -- письма в соответствии с тем, в какой сегмент попали клиенты. Через несколько месяцев сравнить группы: кто больше покупает, кто чаще открывает письма, где меньше отток и т.д. Понятно, что тут много нюансов, которые нужно учесть, а кроме того, на это нужно время.

В качестве более простой и быстрой метрики мы использовали контрольную группу: сегментировали клиентов прошлого -- т.е. по состоянию на момент времени в прошлом, например, полгода назад. А затем смотрели, как ведут себя эти клиенты в будущем (в смысле, после того самого момента в прошлом). Ведь смысл RFM-анализа в том, чтобы на основании исторического поведения пользователей определить их будущее поведение. Результаты получились вполне адекватными -- в одних сегентах люди в будущем покупали чаще, в других реже и т.д. Тестирование прошло успешно. Впрочем, основная заслуга тут конечно тех самых ребят, которые сидя за большим круглым столом придумали это слово - RFM.

### Что дальше

Дальше -- совершенство.

С тех пор как в компании произнесли впервые *Machine Learning*, общей целью стала **БЗК** -- *Большая зеленая кнопка*. Это такая кнопка во весь экран, при нажатии на которую все работает само по себе и приносит хорошую прибыль.

Если говорить про данный проект -- RFM, то его конечная цель -- *Маленькая зеленая кнопка*. То есть, когда человек на нее нажимает, сегменты выделяются автоматически и людям начинают отправляться письма (ну, например). Первая часть уже есть, дело за малым.

В общем, хотим мы, чтобы готовые сегменты машинкой анализировались и ей же решалось, кому скидку дать, кому рекламку послать, кому ничего...




